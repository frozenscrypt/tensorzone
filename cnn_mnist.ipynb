{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2 Layer CNN for MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf                                     #For Machine Learning\n",
    "from tensorflow.examples.tutorials.mnist import input_data  #MNIST data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "MNIST = input_data.read_data_sets('MNIST_data/', one_hot = True)    #Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Size of batch to process at a time\n",
    "batch_size = 128\n",
    "#Learning rate for optimization\n",
    "learning_rate = 1e-3\n",
    "#Number of times to run the model\n",
    "n_epochs = 1\n",
    "#Number o fully connected neurons\n",
    "n_fc_units = 1024\n",
    "#Dropout regularization strength\n",
    "dropout_strength = 0.5\n",
    "#Number of classes for classification\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dropout strength is variable at training and test time, so we define at first a placeholder for dropout\n",
    "dropout = tf.placeholder(dtype = tf.float32, name = 'dropout_strength')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Placeholders for batch input data\n",
    "with tf.name_scope('data'):\n",
    "    X = tf.placeholder(dtype = tf.float32, shape = [None, 784], name = 'X_placeholder')\n",
    "    Y = tf.placeholder(dtype = tf.float32, shape = [None, 10], name = 'Y_placeholder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Variable scope binding for the first conv layer\n",
    "with tf.variable_scope('conv1') as scope:\n",
    "    #Reshape images from one hot vector back to matrices of 28x28\n",
    "    images = tf.reshape(X, shape = [-1,28,28,1], name = 'images')\n",
    "    #Shape of kernels = 5x5, no.of input channels = 1, no. of output channels = 32 \n",
    "    kernels = tf.get_variable(name = 'kernels', shape = [5,5,1,32],\n",
    "                             initializer = tf.truncated_normal_initializer())\n",
    "    #Biases for 32 kernels\n",
    "    biases = tf.get_variable(name = 'biases', shape = [32],\n",
    "                            initializer = tf.random_normal_initializer())\n",
    "    #First Conv layer  , padding = 'SAME' prevents dimensions of the input i.e 28x28\n",
    "    conv = tf.nn.conv2d(images, kernels, strides = [1,1,1,1], padding = 'SAME')\n",
    "    #Activation at first conv layer\n",
    "    conv1 = tf.nn.relu(conv + biases, name = scope.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Max Pooling after first activation layer, output dim: 14*14,no.of channels = 32 \n",
    "with tf.variable_scope('pool1') as scope:\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME', name = 'pool1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Variable scope binfding for Second conv layer\n",
    "with tf.variable_scope('conv2') as scope:\n",
    "    #Kernels of shape 5x5, no.of input channels = 32, no.of output channels = 64\n",
    "    kernels = tf.get_variable(name = 'kernels', shape = [5,5,32,64],\n",
    "                             initializer = tf.truncated_normal_initializer())\n",
    "    #Biases for 64 kernels\n",
    "    biases = tf.get_variable(name = 'biases', shape = [64],\n",
    "                            initializer = tf.random_normal_initializer())\n",
    "    #Second conv layer, padding = 'SAME' prevents dimensions of 14x14\n",
    "    conv = tf.nn.conv2d(pool1, kernels, strides = [1,1,1,1], padding = 'SAME')\n",
    "    #Activation at second conv layer\n",
    "    conv2 = tf.nn.relu(conv + biases, name = scope.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Max pooling after second activation layer, gives output of dim: 7x7, no.of channels = 64\n",
    "with tf.variable_scope('pool2') as scope:\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME', name = 'pool2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Fully connected layer\n",
    "with tf.variable_scope('fc') as scope:\n",
    "    #set of inputs feeding to the FC layer\n",
    "    input_features = 7*7*64\n",
    "    #shape of weight matrix = 7*7*64 x 1024\n",
    "    w = tf.get_variable(name = 'weights', shape = [input_features, n_fc_units],\n",
    "                       initializer = tf.truncated_normal_initializer())\n",
    "    #biases for each 1024 FC layer units\n",
    "    b = tf.get_variable(name = 'biases', shape = [n_fc_units], \n",
    "                       initializer = tf.constant_initializer(0.0))\n",
    "    #Reshape pool2 to shape 1x (7*7*64) ro matrix multiplication\n",
    "    pool2 = tf.reshape(pool2, shape = [-1, input_features])\n",
    "    #Activation layer\n",
    "    fc = tf.nn.relu(tf.matmul(pool2,w) + b, name = 'relu')\n",
    "    #dropout of FC units\n",
    "    fc = tf.nn.dropout(fc, keep_prob = dropout, name = 'relu_dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Final Softmax layer\n",
    "with tf.variable_scope('softmax_layer') as scope:\n",
    "    #weight matrix of shape 1024x10\n",
    "    w = tf.get_variable(name = 'weights', shape = [n_fc_units, num_classes],\n",
    "                       initializer = tf.truncated_normal_initializer())\n",
    "    #biases for each 10 output units\n",
    "    b = tf.get_variable(name = 'biases', shape = [num_classes], \n",
    "                       initializer = tf.constant_initializer(0.0))\n",
    "    #Output for each neuron\n",
    "    logits = tf.matmul(fc, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loss Definition\n",
    "with tf.name_scope('loss'):\n",
    "    #Softmax entropy of class scores\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits(labels = Y, logits = logits)\n",
    "    #loss\n",
    "    loss = tf.reduce_mean(entropy, name = 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('summaries'):\n",
    "    tf.summary.scalar('loss',loss)\n",
    "    tf.summary.histogram('loss', loss)\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Global step to keep count of number of updates made\n",
    "global_step = tf.Variable(0, dtype = tf.int32, trainable = False, name = 'global_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Optimizer for the NN\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss, global_step = global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/aeros/GitHubRepos/tensorzone/checkpoints/convnet_mnist/mnist-convnet-419\n",
      "Loss at step 10: 137.3 \n",
      "Loss at step 20:  69.5 \n",
      "Loss at step 30:  51.2 \n",
      "Loss at step 40:  33.3 \n",
      "Loss at step 50:  27.7 \n",
      "Loss at step 60:  19.3 \n",
      "Loss at step 70:  21.3 \n",
      "Loss at step 80:  15.9 \n",
      "Loss at step 90:  12.8 \n",
      "Loss at step 100:  12.8 \n",
      "Loss at step 110:  10.6 \n",
      "Loss at step 120:  10.3 \n",
      "Loss at step 130:   9.7 \n",
      "Loss at step 140:   8.4 \n",
      "Loss at step 150:   6.9 \n",
      "Loss at step 160:   8.0 \n",
      "Loss at step 170:   6.2 \n",
      "Loss at step 180:   7.1 \n",
      "Loss at step 190:   6.9 \n",
      "Loss at step 200:   6.7 \n",
      "Loss at step 210:   4.6 \n",
      "Loss at step 220:   5.4 \n",
      "Loss at step 230:   4.9 \n",
      "Loss at step 240:   5.7 \n",
      "Loss at step 250:   4.5 \n",
      "Loss at step 260:   4.6 \n",
      "Loss at step 270:   3.8 \n",
      "Loss at step 280:   4.4 \n",
      "Loss at step 290:   3.4 \n",
      "Loss at step 300:   3.3 \n",
      "Loss at step 310:   3.3 \n",
      "Loss at step 320:   2.8 \n",
      "Loss at step 330:   3.4 \n",
      "Loss at step 340:   2.8 \n",
      "Loss at step 350:   3.1 \n",
      "Loss at step 360:   3.1 \n",
      "Loss at step 370:   3.0 \n",
      "Loss at step 380:   2.5 \n",
      "Loss at step 390:   3.2 \n",
      "Loss at step 400:   2.4 \n",
      "Loss at step 410:   1.8 \n",
      "Loss at step 420:   2.3 \n",
      "Optimization finished!\n",
      "Time taken: 485.2117385864258\n",
      "Accuracy = 0.9555\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    #For tensorboard visualizations\n",
    "    writer = tf.summary.FileWriter('graphs/convnet', sess.graph)\n",
    "    #Check if checkpoint present \n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('/checkpoints/convnet_mnist/checkpoint'))\n",
    "    #Restore the latest checkpoint if present\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "    #Training Time\n",
    "    n_batches = int(MNIST.train.num_examples/batch_size)\n",
    "    loss_sum = 0.0\n",
    "    start = time.time()\n",
    "    for i in range(0,n_epochs*n_batches):\n",
    "        X_batch, Y_batch = MNIST.train.next_batch(batch_size)\n",
    "        opt, loss_batch, summary = sess.run([optimizer, loss, summary_op], feed_dict = {X: X_batch,\n",
    "                                                                                       Y: Y_batch,\n",
    "                                                                                       dropout: dropout_strength})\n",
    "        loss_sum += loss_batch\n",
    "        #Adding data to tensorboard summary\n",
    "        writer.add_summary(summary, global_step=i)\n",
    "        if (i+1)%10 == 0:\n",
    "            print( \"Loss at step {}: {:5.1f} \".format(i+1, loss_sum/i))\n",
    "            saver.save(sess, '/checkpoints/convnet_mnist/mnist-convnet', i)\n",
    "            loss_sum = 0.0\n",
    "    print('Optimization finished!')\n",
    "    print('Time taken: {}'.format(time.time() - start))\n",
    "    \n",
    "    #Testing time\n",
    "    n_batches = int(MNIST.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(0, n_batches):\n",
    "        X_batch, Y_batch = MNIST.test.next_batch(batch_size)\n",
    "        opt, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict = {X: X_batch,\n",
    "                                                                                        Y: Y_batch,\n",
    "                                                                                        dropout: 1.0})\n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_preds = tf.equal(tf.argmax(preds,1), tf.argmax(Y_batch,1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
    "        total_correct_preds += sess.run(accuracy)\n",
    "        \n",
    "    print(\"Accuracy = {}\". format(total_correct_preds/MNIST.test.num_examples))\n",
    "        \n",
    "    \n",
    "writer.close()  \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
