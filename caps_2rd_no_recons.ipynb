{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Capsule Network routing twice without reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import tensorflow as tf   #For Machine Learning\n",
    "import numpy as np        #For Mathematial Operations\n",
    "from tensorflow.examples.tutorials.mnist import input_data       #Import MNIST data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Resetting default tensorflow session\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Session Object\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#MNIST dataset\n",
    "MNIST = input_data.read_data_sets('MNIST_data/', one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1        #Number of epochs to train\n",
    "batch_size = 128    #Size of batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Safer Implementation of squashing mentioned in Capsule Net paper. Safer because in case the arr becomes zero then there\n",
    "#will be divide by zero error.\n",
    "def squash(arr,name,axis=-1,keep_dims=False,epsilon=1e-7):\n",
    "    with tf.name_scope(name):\n",
    "        squared_norm = tf.reduce_sum(tf.square(arr), axis=axis, keep_dims=keep_dims, name = 'squared_norm')\n",
    "        safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "        unit_vector = arr/safe_norm\n",
    "        safe_factor = safe_norm/(1.0 + safe_norm)\n",
    "        return safe_factor*unit_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Safe normalizer to get predicted probabilited for each class from secondary capsules\n",
    "def norm(arr,name,axis=-1,epsilon=1e-7,keep_dims=False):\n",
    "    with tf.name_scope(name):\n",
    "        arr_sum = tf.reduce_sum(tf.square(arr),axis=axis,keep_dims=keep_dims,name=name+'sum')\n",
    "        return tf.sqrt(arr_sum + epsilon,name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Placeholders for inputs\n",
    "with tf.name_scope('placeholders'):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None,784], name='X_placeholder')\n",
    "    Y = tf.placeholder(dtype=tf.float32, shape=[None,10], name='Y_placeholder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The first convolutional layer\n",
    "with tf.variable_scope('conv_1') as scope:\n",
    "    #Reshape input to create images\n",
    "    images = tf.reshape(X, shape=[-1,28,28,1], name='images')\n",
    "    #256, 9x9x1 kernels\n",
    "    kernels = tf.get_variable(name='kernels', shape=[9,9,1,256], dtype=tf.float32,\n",
    "                             initializer = tf.truncated_normal_initializer())\n",
    "    #Convolutional layer with VALID padding\n",
    "    conv = tf.nn.conv2d(images, kernels, strides=[1,1,1,1], padding='VALID')\n",
    "    #First conv layer output after relu\n",
    "    conv1 = tf.nn.relu(conv, name=scope.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Second Convolutional layer\n",
    "with tf.variable_scope('conv_2') as scope:\n",
    "    #256, 9x9x256 kernels\n",
    "    kernels = tf.get_variable(name='kernels', shape=[9,9,256,256], dtype=tf.float32,\n",
    "                             initializer=tf.truncated_normal_initializer())\n",
    "    #Convolutional layer with strides at 2 and VALID padding\n",
    "    conv = tf.nn.conv2d(conv1, kernels, strides=[1,2,2,1], padding='VALID')\n",
    "    #Second conv layer output after relu \n",
    "    conv2 = tf.nn.relu(conv, name=scope.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primary capsules layer\n",
    "with tf.variable_scope('primary_capsules') as scope:\n",
    "    #Reshape conv2 layer to form the primary capsules shaped\n",
    "    conv2_reshape = tf.reshape(conv2, shape=[-1,1152,8], name='conv2_reshape')\n",
    "    #Since each capsule denotes probabilities in depth dimension, it cannot be greater than 1. So squash the output to get\n",
    "    #primary capsules\n",
    "    caps_1 = squash(conv2_reshape, keep_dims=True, name=scope.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Weight definitions for secondary capsule computation\n",
    "with tf.variable_scope('weights'):\n",
    "    #Weight matrix initialization\n",
    "    W_raw = tf.get_variable(name='W_raw', shape=[1,1152,10,16,8], dtype=tf.float32,\n",
    "                           initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "    #Repeating weight matrix for the entire batch\n",
    "    W_tiled = tf.tile(W_raw,[batch_size,1,1,1,1],name='W_tiled')\n",
    "    #Expand dimensions of caps_1 \n",
    "    caps_1_expanded = tf.expand_dims(caps_1, axis=-1, name='caps_1_expanded')\n",
    "    #Create a tile of caps_1_expanded by expanding dimension for units in secondary capsules\n",
    "    caps_1_tile = tf.expand_dims(caps_1_expanded, axis=2, name='caps_1_tile')\n",
    "    #Repeating caps_1_tile for the each capsule in second layer\n",
    "    caps_1_output = tf.tile(caps_1_tile,[1,1,10,1,1], name='caps_1_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Computing secondary capsules\n",
    "with tf.variable_scope('secondary_capsules'):\n",
    "    caps_2 = tf.matmul(W_tiled,caps_1_output,name='caps_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round 1 of dynamc routing\n",
    "with tf.name_scope('round_1'):\n",
    "    #Raw weights for each capsule connection: 1152x10\n",
    "    raw_weights_round1 = tf.zeros(shape=[batch_size,1152,10,1,1], dtype=tf.float32, name='raw_weights_round1')\n",
    "    #Routing weights through softmax\n",
    "    routing_weights_round1 = tf.nn.softmax(raw_weights_round1, dim=2, name='routing_weights_round1' )\n",
    "    #Weighted predictions by secondary capsules\n",
    "    weighted_preds_round1 = tf.multiply(routing_weights_round1,caps_2,name='weighted_preds_round1')\n",
    "    #Weighted sum of weighted predictions\n",
    "    weighted_sum_round1 = tf.reduce_mean(weighted_preds_round1,axis=1,keep_dims=True,name='weighted_sum_round1')\n",
    "    #Since they are probabilites, we need to squash them to get secondary capsules output for round 1\n",
    "    caps2_output_round1 = squash(weighted_sum_round1,axis=-2,keep_dims=True,name='caps2_output_round1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Computing agreement between primary and secondary capsules\n",
    "with tf.variable_scope('agreement'):\n",
    "    caps2_output_tiled = tf.tile(caps2_output_round1,[1,1152,1,1,1],name='caps2_output_tiled')\n",
    "    agreement = tf.matmul(caps_2,caps2_output_tiled,transpose_a=True, name='agreement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dynamic routing round 2\n",
    "with tf.name_scope('round2'):\n",
    "    #Raw weights for round 2 is addition of raw_weights for round 1 and agreement\n",
    "    raw_weights_round2 = tf.add(raw_weights_round1,agreement,name='raw_weights_round2')\n",
    "    #Computing routing weights for round 2\n",
    "    routing_weights_round2 = tf.nn.softmax(raw_weights_round2,dim=2,name='routing_weights_round2')\n",
    "    #Weighted predictions\n",
    "    weighted_preds_round2 = tf.multiply(routing_weights_round2,caps_2,name='weighted_preds_round2')\n",
    "    #Weighted sum\n",
    "    weighted_sum_round2 = tf.reduce_sum(weighted_preds_round2,axis=1,keep_dims=True,name='weighted_sum_round2')\n",
    "    #Secondary capsule output for round 2\n",
    "    caps2_output_round2 = squash(weighted_sum_round2,axis=-2,keep_dims=True,name='caps2_output_round2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caps2_output = caps2_output_round2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Probabilites for each class in the output\n",
    "y_probs = norm(caps2_output,name='y_probs',axis=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reducing dimsensions to (batch_size,10)\n",
    "y_probs_final = tf.squeeze(y_probs,[1,-1],name='y_probs_squeezed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameters in loss computation\n",
    "m_plus = 0.9\n",
    "m_minus = 0.1\n",
    "lambda_ = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#One-hot vector of output labels for computing loss\n",
    "T = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part one of margin loss\n",
    "present_error = tf.square(tf.maximum(0.,m_plus - y_probs_final),name='present_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part two of margin loss\n",
    "absent_error = tf.square(tf.maximum(0.,y_probs_final - m_minus),name='absent_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#margin loss for each sample in batch\n",
    "margin_loss_raw = tf.add(T*present_error,lambda_*(1 - T)*absent_error, name='margin_loss_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Total batch loss\n",
    "margin_loss = tf.reduce_mean(tf.reduce_sum(margin_loss_raw,axis=1),name='margin_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Optimizer for model\n",
    "optimizer = tf.train.AdamOptimizer().minimize(margin_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The Process\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#Graph writer for tensorboard\n",
    "writer = tf.summary.FileWriter('/graphs',sess.graph)\n",
    "#Saver object to save and restore\n",
    "saver = tf.train.Saver()\n",
    "#Fetch checkpoint if present\n",
    "ckpt = tf.train.get_checkpoint_state(os.path.dirname('/checkpoints/caps_net/checkpoint'))\n",
    "#Restore model if checkpoint present\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "#Training\n",
    "n_batches = int(MNIST.train.num_examples/batch_size)\n",
    "loss_sum = 0.0\n",
    "start = time.time()\n",
    "for i in range(n_epochs*n_batches):\n",
    "    X_batch, Y_batch = MNIST.train.next_batch(batch_size)\n",
    "    opt,loss = sess.run([optimizer, margin_loss], {X:X_batch, Y:Y_batch})\n",
    "    \n",
    "    if (i+1)%10==0:\n",
    "        print( \"Loss at step {}: {:5.1f} \".format(i+1, loss_sum/i))\n",
    "        saver.save(sess, '/checkpoints/caps_net/checkpoint', i)\n",
    "        loss_sum = 0.0\n",
    "print('Optimization finished')\n",
    "print('Time taken: {}'.format(time.time() - start))\n",
    "\n",
    "#testing\n",
    "total_correct_preds = 0.0\n",
    "n_batches = int(MNIST.test.num_examples/batch_size)\n",
    "for i in range(n_batches):\n",
    "    X_batch, Y_batch = MNIST.test.next_batch(batch_size)\n",
    "    opt, loss, y_preds = sess.run([optimizer, margin_loss, y_probs_final], {X:X_batch, Y_batch})\n",
    "    correct_preds = tf.equal(tf.argmax(y_preds,1),tf.argmax(Y_batch,1))\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct_preds,tf.float32))\n",
    "    total_correct_preds += sess.run(accuracy)\n",
    "print(\"Accuracy = {}\". format(total_correct_preds/MNIST.test.num_examples))\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
