{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Word2Vec SkipGram  implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf            #For Machine Learning\n",
    "import numpy as np                 #For Mathematical Operations\n",
    "import zipfile                     #To load data\n",
    "import collections\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to read data from a dataset text8. The dataset consists of sentences. The function\n",
    "#tokenizes each word in the sentence and returns a list consisting of all words in order\n",
    "#from all sentences removing any special characters, quotes, commas, and other symbols.\n",
    "\n",
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "         data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load data into a variable\n",
    "words = read_data('/home/aeros/Documents/Datasets/text8.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Number of words in dataset\n",
    "n_words = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Size of vocabulary we'll be using\n",
    "vocabulary_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(words):\n",
    "    #Counting each token in the data \n",
    "    count = [['UNK',-1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    \n",
    "    #Creating a word to int dictionary\n",
    "    dictionary = {}\n",
    "    for word, cnt in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "        \n",
    "    #Creating a list of data words represented by their integer value in dictionary\n",
    "    data =[]\n",
    "    unk_count = 0\n",
    "    for word in words: \n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            #if word not in dictionary it is 'UNK' word\n",
    "            index = 0\n",
    "            unk_count += 1   #Increment counter for unknown words\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count   #replace count of unknown words\n",
    "    \n",
    "    #Creating a int to word dictionary\n",
    "    reversed_dictionary = dict(zip(dictionary.values(),dictionary.keys()))\n",
    "    \n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "#Preparing a list of words mapped to integers to feed to the model as input. Also, keeping \n",
    "#count of all tokens, creating a mapping from words to integers and vice versa.\n",
    "\n",
    "def generate_batch(data, batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    #Keeping parameters consistent\n",
    "    assert batch_size%num_skips == 0\n",
    "    assert num_skips <= 2*skip_window\n",
    "    \n",
    "    batch = np.ndarray(shape = [batch_size], dtype = np.int32)\n",
    "    labels = np.ndarray(shape = [batch_size,1], dtype = np.int32)\n",
    "    \n",
    "    #Spaning window for inital buffer space\n",
    "    span = 2*skip_window + 1\n",
    "    \n",
    "    #Buffer to hold part of elements in a list\n",
    "    buffer = collections.deque(maxlen = span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1)%len(data)\n",
    "    \n",
    "    #Batch and label assignment\n",
    "    for i in range(batch_size//num_skips):#batch_size//num_skips gives no. of iterations to update the buffer center\n",
    "        target = skip_window\n",
    "        targets_to_avoid = [skip_window]\n",
    "        \n",
    "        for j in range(num_skips):#No of elements to form groups with the target i.e skip_window or buffer_mid\n",
    "            while target in targets_to_avoid:\n",
    "                target = np.random.randint(0,span)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i*num_skips + j] = buffer[skip_window]\n",
    "            labels[i*num_skips + j,0] = buffer[target] \n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "        \n",
    "    return batch, labels\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get dataset of integers for words\n",
    "data, count, dictionary, reversed_dictionary = build_dataset(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting up parameters for training\n",
    "embed_size = 300       #size of hidden layer\n",
    "num_sampled = 64       #number of sampled examples in nce_loss\n",
    "learning_rate = 1.0    #learning rate for updates\n",
    "batch_size = 128       #processing data in a batch of 128\n",
    "skip_window = 2        #number of steps to look on left and right of center word \n",
    "num_skips = 4          #maximum pairs that could be formed from a center word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Setting up variables for similarity comparison\n",
    "valid_size = 16    #number of random samples to compare\n",
    "valid_window = 100   #window from which the samples are drawn\n",
    "valid_examples = np.random.choice(valid_window, valid_size)    #integer array of 16 random integers\n",
    "valid_dataset = tf.constant(valid_examples, dtype = tf.int32)  #converting to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Number of epochs to train the model\n",
    "n_epochs = 100001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Selecting device for training \n",
    "with tf.device('/cpu:0'):\n",
    "    #Defining name scope for input\n",
    "    with tf.name_scope('inputs'):\n",
    "        #target and context of the skip_gram model\n",
    "        center_words = tf.placeholder(dtype = tf.int32, shape = [batch_size],name = 'center_words')\n",
    "        target_words = tf.placeholder(dtype = tf.int32, shape = [batch_size,1], name = 'target_words')\n",
    "    #Defining name scpe for embedding weights\n",
    "    with tf.name_scope('embed'):\n",
    "        #Embedding matrix for the model\n",
    "        embed_matrix = tf.Variable(tf.random_uniform([vocabulary_size, embed_size], -1, 1), name = 'embed_matrix')\n",
    "    #Defining name scope for loss     \n",
    "    with tf.name_scope('loss'):\n",
    "        embed = tf.nn.embedding_lookup(embed_matrix, center_words, name = 'embed')\n",
    "        #weights for the final layer before nce_loss\n",
    "        nce_weight = tf.Variable(tf.truncated_normal(shape = [vocabulary_size, embed_size], \n",
    "                                                     stddev = 1.0/math.sqrt(embed_size)), name = 'nce_weight')\n",
    "        #biases for the final layer before nce_loss\n",
    "        nce_bias = tf.Variable(tf.zeros(shape = [vocabulary_size]), name = 'nce_bias')\n",
    "        #loss function for the model\n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(weights = nce_weight, biases = nce_bias, labels = target_words,\n",
    "                                             inputs = embed, num_sampled = num_sampled, num_classes = vocabulary_size)\n",
    "                             ,name = 'loss')\n",
    "    #Optimizer for the model\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "    \n",
    "    \n",
    "    #Normalizing embeddings\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embed_matrix), 1, keep_dims = True))\n",
    "    normalized_embeddings = embed_matrix/norm\n",
    "    #looking for embeddings over finite samples\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    #similarity comparison over embeddings\n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialzing variables variable\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized\n",
      "Average loss at step  0 :  255.583633423\n",
      "nearest to their: eliminating, wanton, roy, borough, peninsula, switch, abyss, weird,\n",
      "nearest to often: reasonable, auk, emanate, favours, burner, shiite, consequentialist, recantation,\n",
      "nearest to to: nicolau, multiple, alkaloid, humans, automation, baseball, mystique, installment,\n",
      "nearest to th: gospels, calvi, silvanus, exceeds, arendt, oklahoma, journals, xj,\n",
      "nearest to i: indicated, wins, gluons, chihuahua, ros, venerated, mundi, resold,\n",
      "nearest to been: corvettes, fall, kokomo, synchronic, farkas, praetorians, impotence, lecoq,\n",
      "nearest to from: dhole, hemoglobin, nlds, ashikaga, employee, rttemberg, conveyed, jacopo,\n",
      "nearest to was: dizzy, automotive, wisely, nine, syntax, polyatomic, spiders, nyu,\n",
      "nearest to during: albright, mention, inlaid, ramp, rosalynn, rockin, dantzig, centric,\n",
      "nearest to use: cistercian, consultant, hobsbawm, carrel, gearbox, extraterrestrials, piacenza, embargo,\n",
      "nearest to the: elway, insecticide, carole, beagle, auckland, macromolecules, brahmaputra, forceful,\n",
      "nearest to six: sticks, leontopithecus, falcons, malarial, slipper, hw, thoroughfare, bridle,\n",
      "nearest to its: band, inmarsat, bem, recruited, capturing, motivations, encoding, cva,\n",
      "nearest to war: symbionts, marketed, imf, contingent, absolute, dilution, butt, present,\n",
      "nearest to would: islands, stew, committing, rationalized, chemotherapeutic, dirkjan, predictive, condoleezza,\n",
      "nearest to no: causal, perceval, chairs, asymptomatic, sabah, illiterate, impractical, instead,\n",
      "Average loss at step  2000 :  113.604210421\n",
      "Average loss at step  4000 :  54.114518507\n",
      "Average loss at step  6000 :  34.7108274826\n",
      "Average loss at step  8000 :  24.921304355\n",
      "Average loss at step  10000 :  19.0766465018\n",
      "nearest to their: borough, the, pseudocode, switch, roy, and, a, methyl,\n",
      "nearest to often: a, and, reginae, debts, zero, multicellular, burner, in,\n",
      "nearest to to: and, in, of, gland, is, a, UNK, victoriae,\n",
      "nearest to th: zero, four, one, gospels, gland, ago, journals, tomorrow,\n",
      "nearest to i: zero, reviews, gland, indicated, pads, wins, pseudocode, in,\n",
      "nearest to been: and, shortcuts, ber, fall, gland, gauge, frames, proton,\n",
      "nearest to from: and, reginae, of, asterism, in, gland, fam, abdali,\n",
      "nearest to was: is, gland, and, asterism, to, orange, potato, in,\n",
      "nearest to during: of, invertebrate, albright, leipzig, dhabi, ayers, mention, two,\n",
      "nearest to use: gland, sign, patagonian, proton, impressive, a, bound, df,\n",
      "nearest to the: a, and, gland, of, UNK, in, one, two,\n",
      "nearest to six: zero, gland, nine, one, asterism, tomorrow, fam, in,\n",
      "nearest to its: asterism, and, the, gland, in, excluding, capturing, band,\n",
      "nearest to war: victoriae, reginae, mourned, absolute, orange, present, stanford, donations,\n",
      "nearest to would: islands, anselm, predecessors, victoriae, finalist, direction, deep, spontaneous,\n",
      "nearest to no: gland, reginae, instead, beckwith, causal, museums, two, braille,\n",
      "Average loss at step  12000 :  15.7071422235\n",
      "Average loss at step  14000 :  12.9170630069\n",
      "Average loss at step  16000 :  11.5147131459\n",
      "Average loss at step  18000 :  9.92422812223\n",
      "Average loss at step  20000 :  9.29576732314\n",
      "nearest to their: the, borough, mathbf, his, a, and, homomorphism, its,\n",
      "nearest to often: mathbf, and, for, that, consequentialist, dominate, aristophanes, multicellular,\n",
      "nearest to to: and, for, mathbf, in, by, that, cc, with,\n",
      "nearest to th: mathbf, zero, four, nine, six, eight, one, homomorphism,\n",
      "nearest to i: mathbf, zero, and, gland, antimatter, pads, pseudocode, wins,\n",
      "nearest to been: was, shortcuts, ber, a, mathbf, corvettes, is, that,\n",
      "nearest to from: and, in, by, of, for, mathbf, gland, zero,\n",
      "nearest to was: is, and, mathbf, one, gland, cc, UNK, a,\n",
      "nearest to during: mathbf, of, two, in, invertebrate, frisian, antoninus, four,\n",
      "nearest to use: gland, mathbf, altenberg, aristophanes, carrel, gearbox, proton, a,\n",
      "nearest to the: a, and, one, gland, mathbf, two, s, homomorphism,\n",
      "nearest to six: nine, zero, mathbf, one, eight, two, gland, three,\n",
      "nearest to its: the, asterism, a, mathbf, gland, one, s, cc,\n",
      "nearest to war: mathbf, victoriae, marketed, mourned, reginae, homomorphism, orange, present,\n",
      "nearest to would: homomorphism, mathbf, cc, islands, zero, to, anselm, victoriae,\n",
      "nearest to no: gland, cc, mathbf, reginae, a, the, sulfide, tagline,\n",
      "Average loss at step  22000 :  8.47618106532\n",
      "Average loss at step  24000 :  8.01881307614\n",
      "Average loss at step  26000 :  7.67187980819\n",
      "Average loss at step  28000 :  7.27937296009\n",
      "Average loss at step  30000 :  6.69065673423\n",
      "nearest to their: his, the, a, its, mathbf, homomorphism, borough, zero,\n",
      "nearest to often: that, mathbf, a, and, to, for, by, circ,\n",
      "nearest to to: and, for, by, with, in, mathbf, that, cc,\n",
      "nearest to th: eight, six, four, mathbf, seven, nine, zero, five,\n",
      "nearest to i: zero, mathbf, UNK, two, four, seven, and, s,\n",
      "nearest to been: was, be, is, shortcuts, and, circ, ber, a,\n",
      "nearest to from: and, in, for, of, by, with, to, gland,\n",
      "nearest to was: is, mathbf, and, gland, UNK, are, circ, zero,\n",
      "nearest to during: of, mathbf, in, for, four, backslash, two, six,\n",
      "nearest to use: gland, agouti, altenberg, mathbf, aristophanes, consultant, cistercian, two,\n",
      "nearest to the: a, his, and, two, mathbf, gland, UNK, one,\n",
      "nearest to six: eight, seven, nine, three, zero, five, four, two,\n",
      "nearest to its: the, his, asterism, s, their, gland, mathbf, circ,\n",
      "nearest to war: mathbf, victoriae, circ, marketed, mourned, reginae, ulyanov, homomorphism,\n",
      "nearest to would: to, homomorphism, zero, cc, mathbf, islands, anselm, that,\n",
      "nearest to no: gland, a, cc, that, his, mathbf, reginae, homogeneous,\n",
      "Average loss at step  32000 :  6.8049198277\n",
      "Average loss at step  34000 :  6.77299368453\n",
      "Average loss at step  36000 :  6.30722810996\n",
      "Average loss at step  38000 :  6.16612761772\n",
      "Average loss at step  40000 :  6.27971028757\n",
      "nearest to their: his, the, its, a, mathbf, and, homomorphism, zero,\n",
      "nearest to often: a, and, that, asbestos, mathbf, for, or, eight,\n",
      "nearest to to: with, and, for, by, in, that, mathbf, or,\n",
      "nearest to th: mathbf, eight, three, five, two, zero, four, nine,\n",
      "nearest to i: mathbf, two, six, four, s, three, one, asbestos,\n",
      "nearest to been: be, was, are, is, shortcuts, circ, he, mathbf,\n",
      "nearest to from: and, in, with, for, of, by, gland, or,\n",
      "nearest to was: is, are, were, mathbf, be, and, as, by,\n",
      "nearest to during: in, of, mathbf, for, backslash, on, s, eight,\n",
      "nearest to use: gland, mathbf, agouti, altenberg, aristophanes, two, and, asbestos,\n",
      "nearest to the: a, his, mathbf, and, one, an, this, s,\n",
      "nearest to six: five, four, eight, three, seven, zero, two, nine,\n",
      "nearest to its: the, his, their, asterism, gland, mathbf, s, a,\n",
      "nearest to war: mathbf, victoriae, circ, marketed, ahimelech, reginae, mourned, ulyanov,\n",
      "nearest to would: to, homomorphism, cc, mathbf, that, anselm, and, is,\n",
      "nearest to no: that, a, gland, cc, mathbf, six, zero, reginae,\n",
      "Average loss at step  42000 :  6.25106508303\n",
      "Average loss at step  44000 :  6.22724834549\n",
      "Average loss at step  46000 :  6.03138645482\n",
      "Average loss at step  48000 :  6.04712880647\n",
      "Average loss at step  50000 :  5.67339955449\n",
      "nearest to their: his, its, the, a, mathbf, and, s, homomorphism,\n",
      "nearest to often: that, asbestos, mathbf, or, to, also, for, circ,\n",
      "nearest to to: by, for, and, that, mathbf, with, cc, gland,\n",
      "nearest to th: eight, zero, nine, seven, five, mathbf, three, six,\n",
      "nearest to i: four, and, mathbf, five, two, six, seven, three,\n",
      "nearest to been: be, was, were, are, is, not, he, shortcuts,\n",
      "nearest to from: in, and, with, by, for, of, mathbf, gland,\n",
      "nearest to was: is, UNK, were, are, mathbf, circ, be, gland,\n",
      "nearest to during: in, of, mathbf, by, and, for, backslash, cc,\n",
      "nearest to use: gland, mathbf, agouti, altenberg, or, which, aristophanes, asbestos,\n",
      "nearest to the: a, his, this, mathbf, gland, UNK, its, an,\n",
      "nearest to six: four, eight, five, seven, three, zero, nine, two,\n",
      "nearest to its: the, their, his, a, mathbf, this, s, gland,\n",
      "nearest to war: mathbf, victoriae, circ, ahimelech, marketed, reginae, mourned, ulyanov,\n",
      "nearest to would: to, homomorphism, cc, mathbf, for, that, or, bcl,\n",
      "nearest to no: that, a, gland, cc, mathbf, five, eight, or,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  52000 :  5.88783019555\n",
      "Average loss at step  54000 :  6.01559590054\n",
      "Average loss at step  56000 :  5.76475006127\n",
      "Average loss at step  58000 :  5.58141971827\n",
      "Average loss at step  60000 :  5.74091485333\n",
      "nearest to their: its, his, the, a, mathbf, callisto, and, this,\n",
      "nearest to often: asbestos, mathbf, to, also, be, that, circ, aristophanes,\n",
      "nearest to to: and, for, cc, mathbf, with, that, gland, or,\n",
      "nearest to th: nine, eight, zero, five, seven, three, six, mathbf,\n",
      "nearest to i: four, mathbf, s, and, UNK, vma, three, two,\n",
      "nearest to been: be, was, were, are, is, he, not, mctaggart,\n",
      "nearest to from: and, in, with, by, for, on, of, mathbf,\n",
      "nearest to was: is, were, be, has, had, are, mathbf, UNK,\n",
      "nearest to during: in, mathbf, of, and, after, callisto, five, circ,\n",
      "nearest to use: akita, gland, agouti, mathbf, altenberg, or, and, aristophanes,\n",
      "nearest to the: a, this, an, his, their, UNK, its, gland,\n",
      "nearest to six: four, five, three, eight, seven, zero, two, nine,\n",
      "nearest to its: their, the, his, this, a, mathbf, callisto, gland,\n",
      "nearest to war: mathbf, victoriae, circ, akita, amok, reginae, nine, four,\n",
      "nearest to would: to, homomorphism, akita, cc, mathbf, can, victoriae, bcl,\n",
      "nearest to no: a, that, the, gland, this, cc, mathbf, six,\n",
      "Average loss at step  62000 :  5.68986393952\n",
      "Average loss at step  64000 :  5.71411665761\n",
      "Average loss at step  66000 :  5.3341850065\n",
      "Average loss at step  68000 :  5.21201457953\n",
      "Average loss at step  70000 :  5.58322588146\n",
      "nearest to their: its, his, the, a, gland, callisto, this, mathbf,\n",
      "nearest to often: mathbf, also, that, not, asbestos, a, which, be,\n",
      "nearest to to: in, with, mathbf, or, cc, gland, it, which,\n",
      "nearest to th: eight, six, nine, seven, zero, mathbf, five, four,\n",
      "nearest to i: four, three, mathbf, two, seven, vma, he, and,\n",
      "nearest to been: was, be, were, he, had, are, not, a,\n",
      "nearest to from: in, and, with, of, on, mathbf, for, gland,\n",
      "nearest to was: is, were, has, had, be, are, he, gland,\n",
      "nearest to during: in, of, mathbf, after, on, zero, and, homomorphism,\n",
      "nearest to use: akita, gland, or, mathbf, agouti, altenberg, three, aristophanes,\n",
      "nearest to the: a, his, this, an, mathbf, gland, its, one,\n",
      "nearest to six: seven, eight, four, five, three, zero, nine, one,\n",
      "nearest to its: their, the, his, a, this, an, mathbf, gland,\n",
      "nearest to war: mathbf, victoriae, circ, akita, amok, mourned, reginae, nine,\n",
      "nearest to would: to, homomorphism, akita, can, cc, may, will, mathbf,\n",
      "nearest to no: a, that, the, gland, this, cc, six, or,\n",
      "Average loss at step  72000 :  5.42660904503\n",
      "Average loss at step  74000 :  5.43716944468\n",
      "Average loss at step  76000 :  5.30392452574\n",
      "Average loss at step  78000 :  5.39393527627\n",
      "Average loss at step  80000 :  5.34278903937\n",
      "nearest to their: its, his, the, this, gland, callisto, mathbf, a,\n",
      "nearest to often: also, mathbf, to, be, asbestos, which, not, that,\n",
      "nearest to to: with, mathbf, it, cc, gland, for, which, or,\n",
      "nearest to th: six, eight, zero, nine, seven, mathbf, three, four,\n",
      "nearest to i: he, UNK, and, mathbf, that, vma, three, four,\n",
      "nearest to been: be, was, were, are, had, he, have, mctaggart,\n",
      "nearest to from: and, in, with, mathbf, gland, of, seven, asterism,\n",
      "nearest to was: is, were, had, has, be, are, UNK, mathbf,\n",
      "nearest to during: in, of, after, mathbf, callisto, homomorphism, backslash, seven,\n",
      "nearest to use: or, akita, gland, mathbf, agouti, altenberg, but, callisto,\n",
      "nearest to the: its, a, his, mathbf, gland, their, akita, this,\n",
      "nearest to six: seven, eight, five, four, three, zero, nine, two,\n",
      "nearest to its: their, the, his, this, a, an, mathbf, gland,\n",
      "nearest to war: mathbf, circ, victoriae, akita, amok, mourned, ulyanov, reginae,\n",
      "nearest to would: to, can, may, homomorphism, akita, will, cc, had,\n",
      "nearest to no: a, that, this, gland, cc, or, six, mathbf,\n",
      "Average loss at step  82000 :  5.42236111951\n",
      "Average loss at step  84000 :  5.23254193175\n",
      "Average loss at step  86000 :  5.19195148933\n",
      "Average loss at step  88000 :  5.25767692339\n",
      "Average loss at step  90000 :  5.18255676591\n",
      "nearest to their: its, his, the, s, mathbf, a, gland, thibetanus,\n",
      "nearest to often: also, not, thibetanus, which, mathbf, or, asbestos, it,\n",
      "nearest to to: thibetanus, mathbf, with, cc, or, from, for, and,\n",
      "nearest to th: eight, zero, mathbf, six, seven, one, nine, three,\n",
      "nearest to i: mathbf, thibetanus, he, five, vma, one, four, that,\n",
      "nearest to been: be, was, were, are, had, thibetanus, have, also,\n",
      "nearest to from: in, and, on, or, with, of, mathbf, thibetanus,\n",
      "nearest to was: is, were, has, had, are, be, thibetanus, by,\n",
      "nearest to during: in, after, mathbf, thibetanus, arctos, for, of, homomorphism,\n",
      "nearest to use: or, akita, gland, mathbf, ursus, agouti, thibetanus, altenberg,\n",
      "nearest to the: a, his, their, an, this, its, mathbf, thibetanus,\n",
      "nearest to six: eight, four, seven, three, five, two, zero, mathbf,\n",
      "nearest to its: their, his, the, an, s, a, this, mathbf,\n",
      "nearest to war: mathbf, circ, thibetanus, victoriae, akita, three, amok, seven,\n",
      "nearest to would: to, can, may, will, akita, homomorphism, had, thibetanus,\n",
      "nearest to no: a, this, thibetanus, that, gland, or, cc, six,\n",
      "Average loss at step  92000 :  5.41603034568\n",
      "Average loss at step  94000 :  5.29203475487\n",
      "Average loss at step  96000 :  5.18273368597\n",
      "Average loss at step  98000 :  5.26677032042\n",
      "Average loss at step  100000 :  5.19704910743\n",
      "nearest to their: its, his, the, s, mathbf, gland, akita, thibetanus,\n",
      "nearest to often: also, which, thibetanus, not, be, asbestos, mathbf, that,\n",
      "nearest to to: thibetanus, not, mathbf, from, but, with, and, for,\n",
      "nearest to th: six, zero, eight, four, nine, seven, mathbf, three,\n",
      "nearest to i: mathbf, thibetanus, he, vma, UNK, five, cfa, seven,\n",
      "nearest to been: be, was, were, are, thibetanus, had, have, not,\n",
      "nearest to from: and, in, with, of, into, on, thibetanus, mathbf,\n",
      "nearest to was: is, were, has, had, thibetanus, be, mathbf, are,\n",
      "nearest to during: in, after, of, mathbf, thibetanus, for, eight, by,\n",
      "nearest to use: akita, gland, or, mathbf, ursus, agouti, thibetanus, altenberg,\n",
      "nearest to the: its, a, their, this, his, mathbf, thibetanus, ursus,\n",
      "nearest to six: eight, seven, five, four, zero, nine, three, two,\n",
      "nearest to its: their, the, his, a, this, an, s, mathbf,\n",
      "nearest to war: mathbf, thibetanus, circ, victoriae, akita, four, amok, three,\n",
      "nearest to would: can, may, will, to, had, akita, could, homomorphism,\n",
      "nearest to no: a, that, this, thibetanus, gland, any, cc, it,\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "    #initialize variables\n",
    "    sess.run(init)\n",
    "    print('initialized')\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    average_loss = 0\n",
    "    for i in range(n_epochs):\n",
    "        #Generate batch of data for input\n",
    "        batch_inputs, batch_labels = generate_batch(data, batch_size = batch_size, num_skips = num_skips, \n",
    "                                                    skip_window = skip_window)\n",
    "        feed_dict = {center_words: batch_inputs, target_words: batch_labels}\n",
    "        opt, loss_value = sess.run([optimizer, loss], feed_dict)\n",
    "        average_loss += loss_value\n",
    "        #Print loss at steps of 2000\n",
    "        if i % 2000 == 0:\n",
    "            if i > 0:\n",
    "                average_loss /= 2000\n",
    "            # the average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print(\"Average loss at step \", i, \": \", average_loss)\n",
    "            average_loss = 0\n",
    "    \n",
    "            \n",
    "        #Print similarities at steps of 10000\n",
    "        if i % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for j in range(valid_size):\n",
    "                valid_word = reversed_dictionary[valid_examples[j]]\n",
    "                top_k = 8\n",
    "                nearest = (-sim[j, :]).argsort()[1:top_k + 1]\n",
    "                log_str = \"nearest to %s:\" % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reversed_dictionary[nearest[k]]\n",
    "                    log_str = \"%s %s,\" % (log_str, close_word)\n",
    "                print(log_str)\n",
    "                \n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "    saver.save(sess,'./models/word2vec.ckpt')        \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](word2vec_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard now shows a much linear model that is interpretable. This is because we used tf.name_scope() while defining our variables. The tf.name_scope() groups contents together in one name defined by the user as it is visbible from inputs, embed, loss etc.  \n",
    "\n",
    "We can also see our learned embeddings plotted by clicking on the Embeddings tab. We can also choose from different representations like SNE, PCA etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
