{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Vanilla network on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import \n",
    "import tensorflow as tf                                         # For Machine Learning\n",
    "from tensorflow.examples.tutorials.mnist import input_data      # MNIST dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Loading MNIST dataset into a variable with one-hot encoding i.e 20x28 image is a single vector of 784 and \n",
    "#each output is a vector of 10 labels with one at the label and zero elsewhere \n",
    "MNIST = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Batch_size, epochs, learning_rate\n",
    "\n",
    "batch_size = 128\n",
    "n_epochs = 25\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define inputs\n",
    "X = tf.placeholder(dtype = tf.float32, shape = [batch_size,784], name = 'images')\n",
    "Y = tf.placeholder(dtype = tf.float32, shape = [batch_size,10], name = 'labels' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#No of hidden units\n",
    "n_hidden_units = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Weights and biases, Here we've used Xavier initialization which works well with relu activation function\n",
    "\n",
    "w1 = tf.get_variable(dtype = tf.float32, shape = [784,n_hidden_units], \n",
    "                     initializer = tf.contrib.layers.xavier_initializer(), name = 'hidden_weights')\n",
    "\n",
    "b1 = tf.Variable(tf.zeros(shape = [1,n_hidden_units]), name = \"hidden_biases\")\n",
    "\n",
    "w2 = tf.get_variable(dtype = tf.float32, shape = [n_hidden_units,10], \n",
    "                     initializer = tf.contrib.layers.xavier_initializer(), name = 'output_weights')\n",
    "\n",
    "b2 = tf.Variable(tf.zeros(shape = [1,10]), name = \"output_biases\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute the hidden layer scores\n",
    "hidden_logits = tf.matmul(X,w1) + b1\n",
    "#Compute activations at hidden layer\n",
    "hidden_entropy = tf.nn.relu(hidden_logits)\n",
    "#Compute scores at output layer\n",
    "logits = tf.matmul(hidden_entropy,w2) + b2\n",
    "#Compute Softmax loss at the output layer\n",
    "output_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loss calculation\n",
    "loss = tf.reduce_mean(output_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Optimizer for the network, We didn't use GradientDescentOptimizer here because it is comparitively slower to converge\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Total Correct Predictions count\n",
    "total_correct_preds = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initializer\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loss per epoch\n",
    "loss_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0 =  0.248375402568\n",
      "Loss at epoch 1 =  0.11257444993\n",
      "Loss at epoch 2 =  0.0999627288123\n",
      "Loss at epoch 3 =  0.0751139060898\n",
      "Loss at epoch 4 =  0.0760831276749\n",
      "Loss at epoch 5 =  0.06545011892\n",
      "Loss at epoch 6 =  0.0613262419699\n",
      "Loss at epoch 7 =  0.059895885775\n",
      "Loss at epoch 8 =  0.0535014577062\n",
      "Loss at epoch 9 =  0.0452394372409\n",
      "Loss at epoch 10 =  0.0514961969113\n",
      "Loss at epoch 11 =  0.0413180849549\n",
      "Loss at epoch 12 =  0.0589651852595\n",
      "Loss at epoch 13 =  0.0469580705179\n",
      "Loss at epoch 14 =  0.0427544364754\n",
      "Loss at epoch 15 =  0.0428658477728\n",
      "Loss at epoch 16 =  0.0488559990582\n",
      "Loss at epoch 17 =  0.0417805833628\n",
      "Loss at epoch 18 =  0.0362437764856\n",
      "Loss at epoch 19 =  0.0470676963222\n",
      "Loss at epoch 20 =  0.041320434305\n",
      "Loss at epoch 21 =  0.0551784383337\n",
      "Loss at epoch 22 =  0.0385405582677\n",
      "Loss at epoch 23 =  0.0395969516026\n",
      "Loss at epoch 24 =  0.0340368059119\n",
      "Accuracy 0.9639\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #To save the model, create Saver object\n",
    "    saver = tf.train.Saver()\n",
    "    #Training Model\n",
    "    n_batches = int(MNIST.train.num_examples/batch_size)       #number of batches to train\n",
    "    for i in range(n_epochs):           # training the network on the dataset for n_epochs \n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = MNIST.train.next_batch(batch_size)   #next_batch generates data batch of batch_size\n",
    "            opt, loss_value = sess.run([optimizer, loss], feed_dict = {X:X_batch, Y:Y_batch})\n",
    "            loss_sum += loss_value\n",
    "        loss_mean = loss_sum/n_batches\n",
    "        print(\"Loss at epoch {} = \".format(i),loss_mean)\n",
    "        loss_sum = 0\n",
    "        \n",
    "    saver.save(sess, './models/model.ckpt')\n",
    "    \n",
    "    #Testing Model\n",
    "    n_batches = int(MNIST.test.num_examples/batch_size)   # test data batches\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = MNIST.test.next_batch(batch_size)\n",
    "        opt, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict = {X:X_batch, Y:Y_batch})\n",
    "        preds = tf.nn.softmax(logits_batch)     #getting predictions for each image\n",
    "        correct_preds = tf.equal(tf.argmax(preds,1), tf.argmax(Y_batch,1))  #Comparing predictions with actual labels\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds,tf.float32))#Computing accuracy by suming over an array of ones\n",
    "        total_correct_preds += sess.run(accuracy)\n",
    "    print(\"Accuracy {}\".format(total_correct_preds/MNIST.test.num_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on training a simple model for classification with right initializations and faster Optimization strategies seems very good. With a few more epochs to train it could go close to 97%. But MNIST classification is a solved problem with state of the art technologies reaching accuracies to about 99.80%. This is as far as we could go on MNIST with a Vanilla Nueral Network. One can try increasing the capacity of the model by adding more layers to the model, combining with dropout and batch normalization , but its unlikely that a Vanilla Neural Network would serve more than 97%. An attempt to add batch_normalization to the previous model is shown below. The accuracy actually reduced!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0 =  1.26931818528\n",
      "Loss at epoch 1 =  0.874514361084\n",
      "Loss at epoch 2 =  0.713222288655\n",
      "Loss at epoch 3 =  0.500095380492\n",
      "Loss at epoch 4 =  0.378616767295\n",
      "Loss at epoch 5 =  0.348061662853\n",
      "Loss at epoch 6 =  0.297546567534\n",
      "Loss at epoch 7 =  0.261044002079\n",
      "Loss at epoch 8 =  0.254309315625\n",
      "Loss at epoch 9 =  0.250619057045\n",
      "Loss at epoch 10 =  0.25709687522\n",
      "Loss at epoch 11 =  0.252009694158\n",
      "Loss at epoch 12 =  0.273056957463\n",
      "Loss at epoch 13 =  0.284037641669\n",
      "Loss at epoch 14 =  0.276851629973\n",
      "Loss at epoch 15 =  0.27925591509\n",
      "Loss at epoch 16 =  0.266370114676\n",
      "Loss at epoch 17 =  0.247087856482\n",
      "Loss at epoch 18 =  0.266512148106\n",
      "Loss at epoch 19 =  0.268455316255\n",
      "Loss at epoch 20 =  0.264345035733\n",
      "Loss at epoch 21 =  0.351741983528\n",
      "Loss at epoch 22 =  0.396248425582\n",
      "Loss at epoch 23 =  0.329770478511\n",
      "Loss at epoch 24 =  0.322122983058\n",
      "Accuracy 0.8732\n"
     ]
    }
   ],
   "source": [
    "#Define inputs\n",
    "X = tf.placeholder(dtype = tf.float32, shape = [batch_size,784], name = 'images')\n",
    "Y = tf.placeholder(dtype = tf.float32, shape = [batch_size,10], name = 'labels' )\n",
    "\n",
    "#No of hidden units\n",
    "n_hidden_units = 2000\n",
    "\n",
    "#Weights and biases, Here we've used Xavier initialization which works well with relu activation function\n",
    "\n",
    "w1 = tf.get_variable(dtype = tf.float32, shape = [784,n_hidden_units], \n",
    "                     initializer = tf.contrib.layers.xavier_initializer(), name = 'hidden_weights')\n",
    "\n",
    "b1 = tf.Variable(tf.zeros(shape = [1,n_hidden_units]), name = \"hidden_biases\")\n",
    "\n",
    "w2 = tf.get_variable(dtype = tf.float32, shape = [n_hidden_units,10], \n",
    "                     initializer = tf.contrib.layers.xavier_initializer(), name = 'output_weights')\n",
    "\n",
    "b2 = tf.Variable(tf.zeros(shape = [1,10]), name = \"output_biases\")\n",
    "\n",
    "#Scale Batch Norm of inputs by gamma1\n",
    "gamma1 = tf.Variable(tf.ones(shape = [batch_size,784]), dtype = tf.float32, trainable = True, name = 'gamma1')\n",
    "#Shift Batch Norm of inouts by beta1\n",
    "beta1 = tf.Variable(tf.ones(shape = [batch_size,784]), dtype = tf.float32, trainable = True, name = 'beta1')\n",
    "#Scale Batch Norm of hidden activations by gamma2\n",
    "gamma2 = tf.Variable(tf.ones(shape = [batch_size, n_hidden_units]), dtype = tf.float32, trainable = True, name = 'gamma2')\n",
    "#Scale Batch Norm of hidden activations by beta2\n",
    "beta2 = tf.Variable(tf.ones(shape = [batch_size,n_hidden_units]), dtype = tf.float32, trainable = True, name = 'beta2')\n",
    "#Scalar to add to Variance\n",
    "epsilon = 1e-3\n",
    "\n",
    "#Compute the hidden layer scores\n",
    "X_input = batch_norm(X,gamma1,beta1,epsilon)\n",
    "hidden_logits = tf.matmul(X_input,w1) + b1\n",
    "#Compute activations at hidden layer\n",
    "hidden_entropy = tf.nn.relu(hidden_logits)\n",
    "#Compute dropout\n",
    "# drop_out = tf.nn.dropout(hidden_entropy, keep_prob = 0.5)/0.5 #Here, we are scaling up the activations because at test\n",
    "#time we'll be using the entire network and not sub-samples\n",
    "#Compute scores at output layer\n",
    "hidden_acts = batch_norm(hidden_entropy,gamma2,beta2,epsilon)\n",
    "logits = tf.matmul(hidden_acts,w2) + b2\n",
    "#Compute Softmax loss at the output layer\n",
    "output_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = Y)\n",
    "\n",
    "#Loss calculation\n",
    "loss = tf.reduce_mean(output_entropy)\n",
    "\n",
    "#Optimizer for the network, We didn't use GradientDescentOptimizer here because it is comparitively slower to converge\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "#Total Correct Predictions count\n",
    "total_correct_preds = 0\n",
    "\n",
    "#Initializer\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#Loss per epoch\n",
    "loss_sum = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #To save the model, create Saver object\n",
    "    saver = tf.train.Saver()\n",
    "    #Training Model\n",
    "    n_batches = int(MNIST.train.num_examples/batch_size)       #number of batches to train\n",
    "    for i in range(n_epochs):           # training the network on the dataset for n_epochs \n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = MNIST.train.next_batch(batch_size)   #next_batch generates data batch of batch_size\n",
    "            opt, loss_value = sess.run([optimizer, loss], feed_dict = {X:X_batch, Y:Y_batch})\n",
    "            loss_sum += loss_value\n",
    "        loss_mean = loss_sum/n_batches\n",
    "        print(\"Loss at epoch {} = \".format(i),loss_mean)\n",
    "        loss_sum = 0\n",
    "        \n",
    "    saver.save(sess, './models/model.ckpt')\n",
    "    \n",
    "    #Testing Model\n",
    "    n_batches = int(MNIST.test.num_examples/batch_size)   # test data batches\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = MNIST.test.next_batch(batch_size)\n",
    "        X_test_input = batch_norm(X_batch,gamma1,beta1,epsilon) \n",
    "        opt, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict = {X:sess.run(X_test_input), Y:Y_batch})\n",
    "        preds = tf.nn.softmax(logits_batch)     #getting predictions for each image\n",
    "        correct_preds = tf.equal(tf.argmax(preds,1), tf.argmax(Y_batch,1))  #Comparing predictions with actual labels\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))#Computing accuracy by suming over an array of ones\n",
    "        total_correct_preds += sess.run(accuracy)\n",
    "    print(\"Accuracy {}\".format(total_correct_preds/MNIST.test.num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm(input_data, gamma, beta, epsilon):\n",
    "    input_tensor = tf.convert_to_tensor(input_data)\n",
    "    batch_mean , batch_var = tf.nn.moments(input_tensor, axes = [0])\n",
    "    X_inputs = tf.nn.batch_normalization(input_tensor, mean = batch_mean, variance = batch_var, offset = beta, scale = gamma, variance_epsilon = epsilon)\n",
    "    return X_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](batch_norm.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
